"""
Music Topic Coordinator

This module coordinates music data from multiple sources:
1. Last.fm (real-time listening history via API)
2. Spotify legacy (historical 2013-2023 JSON exports)

Enriches with Spotify API metadata (artist/track info, genres, audio features).
Generates website-optimized files with listening statistics.
Uploads results to Google Drive.

Output: files/topic_processed_files/music/music_processed.csv
        files/website_files/music/music_page_data.csv
"""

import os
import sys
import pandas as pd
import numpy as np
from dotenv import load_dotenv

# Add project root to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))
sys.path.append(project_root)

from src.utils.utils_functions import enforce_snake_case, record_successful_run
from src.utils.drive_operations import upload_multiple_files, verify_drive_connection
from src.utils.spotify_api_utils import spotify_authentication, get_artist_info, get_track_info
from src.topic_processing.music.genre_mapping import get_simplified_genre
from src.topic_processing.website_maintenance.website_maintenance_processing import full_website_maintenance_pipeline
from src.sources_processing.lastfm.lastfm_processing import full_lastfm_pipeline

load_dotenv()


# ============================================================================
# DATA LOADING AND MERGING
# ============================================================================

def load_source_data():
    """
    Load data from source processors.

    Returns:
        pandas.DataFrame: Combined data from Last.fm and Spotify sources
    """
    print("\nüìä Loading source data...")

    lastfm_path = 'files/source_processed_files/lastfm/lastfm_processed.csv'
    spotify_path = 'files/source_processed_files/spotify/spotify_processed.csv'

    dfs_to_merge = []

    # Load Last.fm data
    if os.path.exists(lastfm_path):
        df_lastfm = pd.read_csv(lastfm_path, sep='|', encoding='utf-8', low_memory=False)
        df_lastfm['timestamp'] = pd.to_datetime(df_lastfm['timestamp'])
        print(f"‚úÖ Loaded {len(df_lastfm):,} tracks from Last.fm")
        dfs_to_merge.append(df_lastfm)
    else:
        print(f"‚ö†Ô∏è  Last.fm source file not found: {lastfm_path}")

    # Load Spotify legacy data
    if os.path.exists(spotify_path):
        df_spotify = pd.read_csv(spotify_path, sep='|', encoding='utf-8', low_memory=False)
        df_spotify['timestamp'] = pd.to_datetime(df_spotify['timestamp'])
        print(f"‚úÖ Loaded {len(df_spotify):,} tracks from Spotify legacy")
        dfs_to_merge.append(df_spotify)
    else:
        print(f"‚ö†Ô∏è  Spotify source file not found: {spotify_path}")

    if not dfs_to_merge:
        print("‚ùå No source data found")
        return pd.DataFrame()

    # Merge all sources
    df_combined = pd.concat(dfs_to_merge, ignore_index=True)

    # Remove duplicates (same song_key + timestamp)
    initial_count = len(df_combined)
    df_combined = df_combined.drop_duplicates(subset=['song_key', 'timestamp'], keep='first')
    final_count = len(df_combined)

    duplicates_removed = initial_count - final_count
    if duplicates_removed > 0:
        print(f"üîÑ Removed {duplicates_removed:,} duplicate tracks")

    # Sort by timestamp (newest first)
    df_combined = df_combined.sort_values('timestamp', ascending=False)

    print(f"‚úÖ Combined dataset: {len(df_combined):,} unique tracks")
    print(f"üìÖ Date range: {df_combined['timestamp'].min()} to {df_combined['timestamp'].max()}")

    return df_combined


# ============================================================================
# SPOTIFY ENRICHMENT
# ============================================================================

def enrich_with_spotify_metadata(df):
    """
    Enrich data with Spotify API metadata (artists, tracks, genres, audio features).

    Args:
        df (pandas.DataFrame): Combined source data

    Returns:
        pandas.DataFrame: Enriched data with Spotify metadata
    """
    print("\nüéß Enriching with Spotify API metadata...")

    # Get Spotify API credentials
    client_id = os.environ.get('Spotify_API_Client_ID')
    client_secret = os.environ.get('Spotify_API_Client_Secret')

    if not client_id or not client_secret:
        print("‚ö†Ô∏è  Spotify API credentials not found in environment variables")
        print("   Proceeding without Spotify enrichment...")
        return df

    # Authenticate with Spotify API
    print("üîê Authenticating with Spotify API...")
    token = spotify_authentication(client_id, client_secret)

    # Prepare unique artists and tracks
    unique_artists = list(df['artist_name'].astype(str).replace("nan", "nan_").unique())
    unique_tracks = list(df['song_key'].astype(str).unique())

    print(f"üìä Found {len(unique_artists):,} unique artists and {len(unique_tracks):,} unique tracks")

    # Define work file paths
    artists_work_file = 'files/work_files/lastfm_work_files/artists_infos.csv'
    tracks_work_file = 'files/work_files/lastfm_work_files/tracks_infos.csv'

    # Count how many are NEW (not in work files) - helps estimate API call time
    existing_artists = set()
    existing_tracks = set()
    if os.path.exists(artists_work_file):
        existing_df = pd.read_csv(artists_work_file, sep='|', low_memory=False)
        existing_artists = set(existing_df['artist_name'].str.lower())
    if os.path.exists(tracks_work_file):
        existing_df = pd.read_csv(tracks_work_file, sep='|', low_memory=False)
        existing_tracks = set(existing_df['song_key'].str.lower())

    new_artists_count = len([a for a in unique_artists if str(a).lower() not in existing_artists])
    new_tracks_count = len([t for t in unique_tracks if str(t).lower() not in existing_tracks])
    print(f"üÜï New artists to fetch: {new_artists_count:,} (cached: {len(existing_artists):,})")
    print(f"üÜï New tracks to fetch: {new_tracks_count:,} (cached: {len(existing_tracks):,})")

    # Get artist information
    print("üé§ Gathering artist information from Spotify API...")
    artist_df = get_artist_info(token, unique_artists, artists_work_file)

    # Get track information
    print("üé∂ Gathering track information from Spotify API...")
    track_df = get_track_info(token, unique_tracks, tracks_work_file)

    # Merge artist info (use lowercase for case-insensitive merge)
    print("üîÑ Merging artist metadata...")
    df['_artist_name_lower'] = df['artist_name'].astype(str).str.lower()
    artist_df['_artist_name_lower'] = artist_df['artist_name'].astype(str).str.lower()
    artist_df_no_name = artist_df.drop(columns=['artist_name'])  # Remove to avoid duplicate column

    df_merge_artist = pd.merge(df, artist_df_no_name, how='left', on='_artist_name_lower')
    df_merge_artist = df_merge_artist.drop(columns=['_artist_name_lower'])

    # Check if artist_artwork_url was merged
    if 'artist_artwork_url' in df_merge_artist.columns:
        non_null = df_merge_artist['artist_artwork_url'].notna().sum()
        print(f"   ‚úÖ artist_artwork_url merged ({non_null:,} non-null values)")
    else:
        print(f"   ‚ö†Ô∏è  artist_artwork_url NOT in artist merge result")

    # Merge track info (only new columns)
    print("üîÑ Merging track metadata...")
    cols_to_use = list(track_df.columns.difference(df_merge_artist.columns))
    cols_to_use.append('song_key')

    # Ensure album_artwork_url is included if it exists in track_df
    if 'album_artwork_url' in track_df.columns and 'album_artwork_url' not in cols_to_use:
        cols_to_use.append('album_artwork_url')

    # Use lowercase song_key for case-insensitive merge (work file stores lowercase keys)
    df_merge_artist['_song_key_lower'] = df_merge_artist['song_key'].astype(str).str.lower()
    track_df_subset = track_df[cols_to_use].copy()
    track_df_subset['_song_key_lower'] = track_df_subset['song_key'].astype(str).str.lower()
    track_df_subset = track_df_subset.drop(columns=['song_key'])  # Remove original to avoid duplicate

    df_enriched = pd.merge(df_merge_artist, track_df_subset, how='left', on='_song_key_lower')
    df_enriched = df_enriched.drop(columns=['_song_key_lower'])  # Clean up temp column

    # Check if album_artwork_url was merged
    if 'album_artwork_url' in df_enriched.columns:
        non_null = df_enriched['album_artwork_url'].notna().sum()
        print(f"   ‚úÖ album_artwork_url merged ({non_null:,} non-null values)")
    else:
        print(f"   ‚ö†Ô∏è  album_artwork_url NOT in track merge result")

    print(f"‚úÖ Enrichment complete: {len(df_enriched.columns)} total columns")

    return df_enriched


# ============================================================================
# LISTENING STATISTICS
# ============================================================================

def compute_completion(df):
    """
    Calculate listening completion percentage and skip detection.

    Completion is calculated by comparing consecutive plays:
    - If time to next track < track duration, track was likely skipped
    - Completion % = time_elapsed / track_duration

    Uses vectorized pandas operations for performance.

    Args:
        df (pandas.DataFrame): Data with timestamp and track_duration columns

    Returns:
        pandas.DataFrame: Data with 'completion' and 'is_skipped_track' columns
    """
    print("\nüìä Calculating listening statistics...")

    df = df.copy()

    # Sort chronologically for time-diff calculation
    df = df.sort_values('timestamp', ascending=True).reset_index(drop=True)

    # Convert track_duration to numeric seconds (handle non-numeric values)
    df['_duration_sec'] = pd.to_numeric(df['track_duration'], errors='coerce') / 1000

    # Calculate time diff to next track (vectorized)
    df['_time_to_next'] = df['timestamp'].shift(-1) - df['timestamp']
    df['_time_to_next_sec'] = df['_time_to_next'].dt.total_seconds()

    # Calculate completion (vectorized) - clip between 0 and 1
    df['completion'] = (df['_time_to_next_sec'] / df['_duration_sec']).clip(0, 1)

    # Handle missing/zero durations (assume full listen)
    df.loc[df['_duration_sec'].isna() | (df['_duration_sec'] == 0), 'completion'] = 1.0

    # Last track - assume completed (no next track to compare)
    df.loc[df.index[-1], 'completion'] = 1.0

    # Fill any remaining NaN completions with 1.0
    df['completion'] = df['completion'].fillna(1.0)

    # Skip detection: True if completion < 1.0 (boolean)
    df['is_skipped_track'] = df['completion'] < 1.0

    # Clean up temp columns
    df = df.drop(columns=['_duration_sec', '_time_to_next', '_time_to_next_sec'])

    # Sort back to descending timestamp
    df = df.sort_values('timestamp', ascending=False).reset_index(drop=True)

    # Calculate and print statistics
    avg_completion = df['completion'].mean() * 100
    skip_rate = df['is_skipped_track'].mean() * 100
    print(f"‚úÖ Average completion: {avg_completion:.1f}%")
    print(f"‚úÖ Skip rate: {skip_rate:.1f}%")

    return df


def calculate_discovery_flags(df):
    """
    Calculate new artist/track discovery flags (boolean columns).

    Flags:
    - is_new_artist: True only for the first listen of an artist
    - is_new_track: True only for the first listen of a track
    - is_new_recurring_artist: True only at 10th listen of an artist (milestone)
    - is_new_recurring_track: True only at 5th listen of a track (milestone)
    - is_recurring_artist: True for ALL listens once artist has 10+ total listens
    - is_recurring_track: True for ALL listens once track has 5+ total listens

    Args:
        df (pandas.DataFrame): Data with timestamp column

    Returns:
        pandas.DataFrame: Data with discovery flag columns (boolean)
    """
    print("üîç Calculating discovery flags...")

    # Sort chronologically for cumulative counting
    df = df.sort_values('timestamp', ascending=True).reset_index(drop=True)

    # First listen flags (True only for the first occurrence)
    df['is_new_artist'] = df.groupby('artist_name').cumcount() == 0
    df['is_new_track'] = df.groupby('track_name').cumcount() == 0

    # Milestone flags (True only at the 10th/5th listen)
    df['is_new_recurring_artist'] = df.groupby('artist_name').cumcount() == 9  # 10th listen (0-indexed)
    df['is_new_recurring_track'] = df.groupby('track_name').cumcount() == 4  # 5th listen (0-indexed)

    # Recurring status (True for ALL listens once threshold reached)
    # Use transform to get total count per artist/track across all time
    artist_total_count = df.groupby('artist_name')['artist_name'].transform('count')
    track_total_count = df.groupby('track_name')['track_name'].transform('count')

    df['is_recurring_artist'] = artist_total_count >= 10
    df['is_recurring_track'] = track_total_count >= 5

    # Sort back to descending timestamp
    df = df.sort_values('timestamp', ascending=False).reset_index(drop=True)

    # Print statistics
    new_artists = df['is_new_artist'].sum()
    new_tracks = df['is_new_track'].sum()
    recurring_artists = df['is_recurring_artist'].sum()
    recurring_tracks = df['is_recurring_track'].sum()
    print(f"‚úÖ Discovered {new_artists:,} new artists and {new_tracks:,} new tracks")
    print(f"‚úÖ Recurring: {recurring_artists:,} artist listens, {recurring_tracks:,} track listens")

    return df


# ============================================================================
# OUTPUT FILE GENERATION
# ============================================================================

def select_output_columns(df):
    """
    Select only columns needed for final output.

    Work files contain ~60+ columns (all Spotify API fields).
    Output files contain ~35 columns (only what's needed for website/analysis).

    Args:
        df (pandas.DataFrame): Full enriched data

    Returns:
        pandas.DataFrame: Data with only output columns
    """
    output_columns = [
        # Core identifiers
        'timestamp', 'song_key', 'artist_name', 'album_name', 'track_name',

        # Album metadata
        'album_release_date', 'album_type',

        # Artist metadata
        'followers', 'followers_total', 'artist_popularity', 'popularity',

        # Genres (first 14 for display, plus JSON for future use)
        'genre_1', 'genre_2', 'genre_3', 'genre_4', 'genre_5', 'genre_6',
        'genre_7', 'genre_8', 'genre_9', 'genre_10', 'genre_11', 'genre_12',
        'genre_13', 'genre_14', 'genres_json',

        # Track metadata
        'track_duration', 'track_popularity', 'track_number', 'explicit',

        # Audio features
        'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',
        'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',

        # Artwork URLs
        'album_artwork_url', 'artist_artwork_url',

        # Listening behavior (calculated fields)
        'completion', 'is_skipped_track',

        # Discovery flags (boolean)
        'is_new_artist', 'is_new_track', 'is_new_recurring_artist', 'is_new_recurring_track',
        'is_recurring_artist', 'is_recurring_track',

        # Source tracking
        'source'
    ]

    # Keep only columns that exist in the dataframe
    existing_output_columns = [col for col in output_columns if col in df.columns]

    # Log how many columns are being filtered out
    removed_count = len(df.columns) - len(existing_output_columns)
    if removed_count > 0:
        print(f"üìä Filtered output: keeping {len(existing_output_columns)} columns, removing {removed_count} work-file-only columns")

    return df[existing_output_columns]


def power_bi_processing(df):
    """
    Data type conversions for better display in analysis tools.

    Args:
        df (pandas.DataFrame): Output data

    Returns:
        pandas.DataFrame: Cleaned data
    """
    df = df.copy()

    # Fill missing genre_1 with 'Unknown'
    if 'genre_1' in df.columns:
        df['genre_1'] = df['genre_1'].fillna('Unknown')

    # Convert track_duration to float (replace non-numeric strings with 0)
    if 'track_duration' in df.columns:
        df['track_duration'] = df['track_duration'].replace(['No API result', 'Unknown', 'Unknown Error'], '0').astype(float)

    return df


# ============================================================================
# WEBSITE FILE GENERATION
# ============================================================================

def generate_music_website_files(df):
    """
    Generate website-optimized files for the Music page.

    Args:
        df (pandas.DataFrame): Processed data (topic-level)

    Returns:
        bool: True if successful, False otherwise
    """
    print("\nüåê Generating website files for Music page...")

    try:
        # Ensure output directory exists
        website_dir = 'files/website_files/music'
        os.makedirs(website_dir, exist_ok=True)

        # Work with copy to avoid modifying original
        df_web = df.copy()

        # Enforce snake_case before saving
        df_web = enforce_snake_case(df_web, "music_page_data")

        # Add toggle_id - unique identifier for each listening event
        df_web['toggle_id'] = range(1, len(df_web) + 1)
        print(f"‚úÖ Added toggle_id column ({len(df_web):,} toggles)")

        # Add listening_seconds - calculated from completion and track_duration
        # listening_seconds = (completion * track_duration_ms) / 1000
        df_web['listening_seconds'] = (df_web['completion'] * df_web['track_duration']) / 1000
        df_web['listening_seconds'] = df_web['listening_seconds'].fillna(0).astype(int)
        print(f"‚úÖ Added listening_seconds column")

        # Combine genres from available genre columns into single 'genres' column
        # Use comma separator (not pipe, which is the CSV delimiter)
        # Filter to only existing genre columns (may have genre_1 through genre_8 or more)
        genre_cols = [f'genre_{i}' for i in range(1, 15) if f'genre_{i}' in df_web.columns]
        if genre_cols:
            df_web['genres'] = df_web[genre_cols].apply(
                lambda row: ', '.join([str(g) for g in row if pd.notna(g) and str(g) != '' and str(g) != 'nan']),
                axis=1
            )
            print(f"‚úÖ Combined genres from {len(genre_cols)} genre columns into single column (comma-separated)")
        else:
            df_web['genres'] = ''
            print(f"‚ö†Ô∏è  No genre columns found, genres will be empty")

        # Add simplified_genre column using genre_1
        df_web['simplified_genre'] = df_web['genre_1'].apply(get_simplified_genre)
        print(f"‚úÖ Created simplified_genre from genre_1 using genre mapping")

        # Select columns for website file
        website_columns = [
            'toggle_id',
            'song_key',
            'artist_name',
            'album_name',
            'track_name',
            'timestamp',
            'album_release_date',
            'followers',
            'artist_popularity',
            'genre_1',
            'simplified_genre',
            'genres',
            'track_duration',
            'track_popularity',
            'completion',
            'is_skipped_track',
            'listening_seconds'
        ]

        # Add discovery flag columns if they exist in the data
        if 'is_new_artist' in df_web.columns:
            website_columns.append('is_new_artist')
            print(f"‚úÖ Including is_new_artist column")

        if 'is_new_track' in df_web.columns:
            website_columns.append('is_new_track')
            print(f"‚úÖ Including is_new_track column")

        # Add is_recurring_artist if it exists in the data
        if 'is_recurring_artist' in df_web.columns:
            website_columns.append('is_recurring_artist')
            print(f"‚úÖ Including is_recurring_artist column")

        # Add is_recurring_track if it exists in the data
        if 'is_recurring_track' in df_web.columns:
            website_columns.append('is_recurring_track')
            print(f"‚úÖ Including is_recurring_track column")

        # Add is_new_recurring_artist if it exists in the data (milestone: 10th listen)
        if 'is_new_recurring_artist' in df_web.columns:
            website_columns.append('is_new_recurring_artist')
            print(f"‚úÖ Including is_new_recurring_artist column")

        # Add is_new_recurring_track if it exists in the data (milestone: 5th listen)
        if 'is_new_recurring_track' in df_web.columns:
            website_columns.append('is_new_recurring_track')
            print(f"‚úÖ Including is_new_recurring_track column")

        # Add album_artwork_url if it exists in the data
        if 'album_artwork_url' in df_web.columns:
            website_columns.append('album_artwork_url')
            non_null = df_web['album_artwork_url'].notna().sum()
            has_url = df_web['album_artwork_url'].str.startswith('http', na=False).sum()
            print(f"‚úÖ Including album_artwork_url column ({non_null:,} non-null, {has_url:,} valid URLs)")
        else:
            print(f"‚ö†Ô∏è  album_artwork_url column NOT found in data")

        # Add artist_artwork_url if it exists in the data
        if 'artist_artwork_url' in df_web.columns:
            website_columns.append('artist_artwork_url')
            non_null = df_web['artist_artwork_url'].notna().sum()
            has_url = df_web['artist_artwork_url'].str.startswith('http', na=False).sum()
            print(f"‚úÖ Including artist_artwork_url column ({non_null:,} non-null, {has_url:,} valid URLs)")
        else:
            print(f"‚ö†Ô∏è  artist_artwork_url column NOT found in data")

        # Filter to only website columns
        df_web = df_web[website_columns]
        print(f"‚úÖ Filtered to {len(website_columns)} website columns")

        # Convert boolean columns to integers for proper CSV storage
        # (Python "True"/"False" strings are truthy in JavaScript, 0/1 works correctly)
        bool_cols = ['is_skipped_track', 'is_new_artist', 'is_new_track',
                     'is_recurring_artist', 'is_recurring_track',
                     'is_new_recurring_artist', 'is_new_recurring_track']
        for col in bool_cols:
            if col in df_web.columns:
                df_web[col] = df_web[col].astype(int)
        print(f"‚úÖ Converted boolean columns to integers")

        # Save website file
        website_path = f'{website_dir}/music_page_data.csv'
        df_web.to_csv(website_path, sep='|', index=False, encoding='utf-8')
        print(f"‚úÖ Website file: {len(df_web):,} records ‚Üí {website_path}")

        return True

    except Exception as e:
        print(f"‚ùå Error generating website files: {e}")
        import traceback
        traceback.print_exc()
        return False


# ============================================================================
# MAIN PROCESSING FUNCTION
# ============================================================================

def create_music_file():
    """
    Main processing function for Music topic coordinator.

    Workflow:
    1. Load data from source processors (Last.fm + Spotify legacy)
    2. Merge and deduplicate
    3. Enrich with Spotify API metadata
    4. Calculate listening statistics (completion, skip detection)
    5. Calculate discovery flags
    6. Select output columns
    7. Generate website files
    8. Save processed data

    Returns:
        bool: True if successful, False otherwise
    """
    print("\n" + "="*70)
    print("üéµ MUSIC TOPIC COORDINATOR PROCESSING")
    print("="*70)

    try:
        # Step 1: Load source data
        df = load_source_data()

        if df.empty:
            print("‚ùå No source data available")
            return False

        # Step 2: Enrich with Spotify metadata
        df = enrich_with_spotify_metadata(df)

        # Step 3: Calculate listening statistics
        df = compute_completion(df)

        # Step 4: Calculate discovery flags
        df = calculate_discovery_flags(df)

        # Step 5: Select output columns (filter work columns)
        df = select_output_columns(df)

        # Step 6: Power BI processing (data type conversions)
        df = power_bi_processing(df)

        # Step 7: Enforce snake_case
        df = enforce_snake_case(df, "music_processed")

        # Step 8: Save topic-level processed file
        output_path = 'files/topic_processed_files/music/music_processed.csv'
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        print(f"\nüíæ Saving topic processed data to {output_path}...")
        df.to_csv(output_path, sep='|', index=False, encoding='utf-8')

        print(f"‚úÖ Successfully processed music data!")
        print(f"üìä Output: {len(df):,} tracks")
        print(f"üìÖ Date range: {df['timestamp'].min()} to {df['timestamp'].max()}")
        print(f"üé§ Unique artists: {df['artist_name'].nunique():,}")
        print(f"üéµ Unique tracks: {df['song_key'].nunique():,}")

        # Show sample
        print(f"\nüìã Sample records:")
        sample_df = df.head(3)[['timestamp', 'artist_name', 'track_name', 'completion', 'is_skipped_track']]
        for _, row in sample_df.iterrows():
            completion_pct = row['completion'] * 100
            skip_status = "‚è≠Ô∏è " if row['is_skipped_track'] else "‚úì"
            print(f"  {skip_status} {str(row['timestamp'])[:16]} | {row['artist_name']} - {row['track_name']} ({completion_pct:.0f}%)")

        # Step 9: Generate website files
        website_success = generate_music_website_files(df)

        if not website_success:
            print("‚ö†Ô∏è  Warning: Website file generation had issues")
            return False

        return True

    except Exception as e:
        print(f"‚ùå Error in music topic processing: {e}")
        import traceback
        traceback.print_exc()
        return False


# ============================================================================
# UPLOAD FUNCTIONS
# ============================================================================

def upload_music_results():
    """
    Upload music results to Google Drive.

    Returns:
        bool: True if successful, False otherwise
    """
    print("\n‚¨ÜÔ∏è  Uploading music results to Google Drive...")

    files_to_upload = ['files/website_files/music/music_page_data.csv']

    # Filter to only existing files
    existing_files = [f for f in files_to_upload if os.path.exists(f)]

    if not existing_files:
        print("‚ùå No files found to upload")
        return False

    print(f"üì§ Uploading {len(existing_files)} file(s)...")
    success = upload_multiple_files(existing_files)

    if success:
        print("‚úÖ Music results uploaded successfully!")
    else:
        print("‚ùå Some files failed to upload")

    return success


# ============================================================================
# PIPELINE FUNCTIONS
# ============================================================================

def full_music_pipeline(auto_full=False, auto_process_only=False):
    """
    Complete Music TOPIC coordinator pipeline.

    Options:
    1. Process source data, enrich with Spotify, generate website files, and upload to Drive
    2. Process existing topic data and upload to Drive
    3. Upload existing processed files to Drive

    Args:
        auto_full (bool): If True, automatically runs option 1 without user input
        auto_process_only (bool): If True, automatically runs option 2 without user input

    Returns:
        bool: True if pipeline completed successfully, False otherwise
    """
    print("\n" + "="*70)
    print("üéµ MUSIC TOPIC COORDINATOR PIPELINE")
    print("="*70)

    if auto_process_only:
        print("ü§ñ Auto process mode: Processing existing data and uploading...")
        choice = "2"
    elif auto_full:
        print("ü§ñ Auto mode: Full processing from source data...")
        choice = "1"
    else:
        print("\nSelect an option:")
        print("1. Process source data, enrich with Spotify, and upload to Drive")
        print("2. Process existing topic data and upload to Drive")
        print("3. Upload existing processed files to Drive")

        choice = input("\nEnter your choice (1-3): ").strip()

    success = False

    if choice == "1":
        print("\nüöÄ Option 1: Full processing from source data...")

        # Step 1: Run Last.fm source pipeline (download + process)
        print("\nüì• Running Last.fm source pipeline...")
        full_lastfm_pipeline(auto_full=True)

        # Step 2: Process music topic data
        process_success = create_music_file()

        if not process_success:
            print("‚ùå Processing failed, skipping upload")
            return False

        # Test drive connection before upload
        if not verify_drive_connection():
            print("‚ö†Ô∏è  Warning: Google Drive connection issues detected")
            proceed = input("Continue with upload anyway? (Y/N): ").upper() == 'Y'
            if not proceed:
                print("‚úÖ Processing completed successfully (upload skipped)")
                return True

        # Upload results
        upload_success = upload_music_results()
        success = upload_success

        if success:
            record_successful_run('topic_music', 'coordination')

    elif choice == "2":
        print("\n‚öôÔ∏è  Option 2: Process existing topic data and upload...")

        # Check if topic processed file exists
        topic_file = 'files/topic_processed_files/music/music_processed.csv'

        if os.path.exists(topic_file):
            # Regenerate website files from existing topic data
            df = pd.read_csv(topic_file, sep='|', encoding='utf-8', low_memory=False)
            website_success = generate_music_website_files(df)

            if not website_success:
                print("‚ùå Website file generation failed")
                return False
        else:
            print(f"‚ùå Topic processed file not found: {topic_file}")
            print("   Run option 1 first to process source data")
            return False

        # Test drive connection before upload
        if not verify_drive_connection():
            print("‚ö†Ô∏è  Warning: Google Drive connection issues detected")
            proceed = input("Continue with upload anyway? (Y/N): ").upper() == 'Y'
            if not proceed:
                print("‚úÖ Processing completed successfully (upload skipped)")
                return True

        # Upload results
        upload_success = upload_music_results()
        success = upload_success

        if success:
            record_successful_run('topic_music', 'coordination')

    elif choice == "3":
        print("\nüì§ Option 3: Upload existing processed files...")

        # Test drive connection before upload
        if not verify_drive_connection():
            print("‚ö†Ô∏è  Warning: Google Drive connection issues detected")
            proceed = input("Continue with upload anyway? (Y/N): ").upper() == 'Y'
            if not proceed:
                print("‚úÖ Upload skipped")
                return True

        # Upload results
        upload_success = upload_music_results()
        success = upload_success

        if success:
            record_successful_run('topic_music', 'coordination')

    else:
        print("‚ùå Invalid choice. Please select 1-3.")
        return False

    # Final status
    print("\n" + "="*70)
    if success:
        print("‚úÖ Music topic coordinator completed successfully!")
        print("üìä Topic output: files/topic_processed_files/music/music_processed.csv")
        print("üåê Website output: files/website_files/music/music_page_data.csv")
        # Update website tracking file
        full_website_maintenance_pipeline(auto_mode=True, quiet=True)
    else:
        print("‚ùå Music topic coordinator failed")
    print("="*70)

    return success


if __name__ == "__main__":
    # Allow running this file directly
    print("üéµ Music Topic Coordinator")
    print("This tool processes music data from multiple sources and enriches with Spotify metadata.")
    print("Note: Run source processors (Last.fm, Spotify) first to generate source data\n")

    # Run the pipeline
    full_music_pipeline(auto_full=False)
