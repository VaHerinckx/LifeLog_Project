import pandas as pd
import numpy as np
import requests
import time
import os
import json
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from src.utils.file_operations import clean_rename_move_file, check_file_exists
from src.utils.web_operations import open_web_urls, prompt_user_download_status
from src.utils.drive_operations import upload_multiple_files, verify_drive_connection

# Fiction genres classification
fiction_genres = ['drama', 'horror', 'thriller']


def load_reading_dates_json():
    """Load reading dates from JSON file"""
    json_path = 'files/work_files/gr_work_files/reading_dates.json'

    if os.path.exists(json_path):
        try:
            with open(json_path, 'r') as f:
                dates_data = json.load(f)
            print(f"üìö Loaded {len(dates_data)} book dates from JSON")
            return dates_data
        except Exception as e:
            print(f"‚ö†Ô∏è  Error loading JSON file: {e}")
            return {}
    else:
        print("üìö No existing reading dates JSON found - creating new one")
        return {}


def save_reading_dates_json(dates_data):
    """Save reading dates to JSON file"""
    json_path = 'files/work_files/gr_work_files/reading_dates.json'
    os.makedirs(os.path.dirname(json_path), exist_ok=True)

    try:
        with open(json_path, 'w') as f:
            json.dump(dates_data, f, indent=2, default=str)
        print(f"üíæ Saved {len(dates_data)} book dates to JSON")
        return True
    except Exception as e:
        print(f"‚ùå Error saving JSON file: {e}")
        return False


def migrate_excel_to_json():
    """One-time migration from Excel file to JSON format"""
    print("üîÑ Checking for Excel to JSON migration...")

    excel_path = 'files/work_files/gr_work_files/gr_dates_input.xlsx'
    json_path = 'files/work_files/gr_work_files/reading_dates.json'

    # If JSON already exists, skip migration
    if os.path.exists(json_path):
        print("‚úÖ JSON file already exists, skipping migration")
        return True

    # If Excel doesn't exist, nothing to migrate
    if not os.path.exists(excel_path):
        print("‚ÑπÔ∏è  No Excel file found to migrate")
        return True

    try:
        print("üìä Migrating Excel data to JSON...")
        df_excel = pd.read_excel(excel_path)

        dates_data = {}
        migrated_count = 0

        for _, row in df_excel.iterrows():
            book_id = str(row.get('Book Id', ''))
            title = row.get('Title', '')

            if book_id and book_id != 'nan' and book_id != '0':
                dates_data[book_id] = {
                    'title': title,
                    'date_started': row.get('Date started', ''),
                    'date_ended': row.get('Date ended', ''),
                    'cover_url': row.get('cover_url', ''),
                    'check_status': row.get('Check', ''),
                    'migrated_from_excel': True
                }
                migrated_count += 1

        if save_reading_dates_json(dates_data):
            print(f"‚úÖ Successfully migrated {migrated_count} books from Excel to JSON")
            return True
        else:
            return False

    except Exception as e:
        print(f"‚ùå Error during migration: {e}")
        return False


def download_goodreads_data():
    """Opens Goodreads export page and prompts user to download data"""
    print("üìö Starting Goodreads data download...")

    urls = ['https://www.goodreads.com/review/import']
    open_web_urls(urls)

    print("üìù Instructions:")
    print("   1. Click 'Export Library'")
    print("   2. Wait for the export to be prepared")
    print("   3. Download the CSV file when ready")
    print("   4. The file will be named 'goodreads_library_export.csv'")

    response = prompt_user_download_status("Goodreads")
    return response


def move_goodreads_files():
    """Moves the downloaded Goodreads file from Downloads to the correct export folder"""
    print("üìÅ Moving Goodreads files...")

    success = clean_rename_move_file(
        export_folder="files/exports/goodreads_exports",
        download_folder="/Users/valen/Downloads",
        file_name="goodreads_library_export.csv",
        new_file_name="gr_export.csv"
    )

    if success:
        print("‚úÖ Successfully moved Goodreads export to exports folder")
    else:
        print("‚ùå Failed to move Goodreads files")

    return success


def extract_detailed_reading_dates_and_cover(driver, view_link, book_title):
    """Click on a view link and extract Start/End reading dates AND book cover URL"""
    try:
        print(f"    üîç Extracting data for '{book_title}'...")

        # Handle potential overlays/banners that might intercept clicks
        try:
            overlay_selectors = [
                ".siteHeader__topFullImage",
                "[class*='banner']",
                "[class*='overlay']",
                "[class*='modal']"
            ]
            for selector in overlay_selectors:
                try:
                    overlay = driver.find_element(By.CSS_SELECTOR, selector)
                    if overlay.is_displayed():
                        driver.execute_script("arguments[0].style.display = 'none';", overlay)
                except:
                    pass
        except:
            pass

        # Try multiple click methods
        click_successful = False

        # Method 1: Scroll and regular click
        try:
            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", view_link)
            time.sleep(1)
            view_link.click()
            click_successful = True
        except Exception as e1:
            # Method 2: JavaScript click
            try:
                driver.execute_script("arguments[0].click();", view_link)
                click_successful = True
            except Exception as e2:
                # Method 3: Navigate directly to the URL
                try:
                    view_url = view_link.get_attribute('href')
                    if view_url:
                        driver.get(view_url)
                        click_successful = True
                except Exception as e3:
                    pass

        if not click_successful:
            print(f"    ‚ùå Could not access detailed view for '{book_title}'")
            return {}

        # Wait for the detailed view to load
        time.sleep(3)

        result = {}

        # Extract reading dates
        try:
            # Look for reading progress elements
            progress_selectors = [
                "//h3[contains(text(), 'READING PROGRESS')]/following-sibling::ul",
                "//h3[contains(text(), 'READING PROGRESS')]/following-sibling::div",
                "//*[contains(text(), 'READING PROGRESS')]/following-sibling::*",
                "//div[contains(@class, 'readingProgress')]",
                "//*[contains(text(), 'Started Reading') or contains(text(), 'Finished Reading')]"
            ]

            progress_items = []
            for selector in progress_selectors:
                try:
                    elements = driver.find_elements(By.XPATH, selector)
                    if elements:
                        for element in elements:
                            progress_items.extend(element.find_elements(By.XPATH, ".//*"))
                        break
                except:
                    continue

            # If we couldn't find a section, search the whole page
            if not progress_items:
                progress_items = driver.find_elements(By.XPATH, "//*[contains(text(), 'Started Reading') or contains(text(), 'Finished Reading')]")

            # Parse the progress items for dates
            for item in progress_items:
                try:
                    text = item.text.strip()

                    if "Started Reading" in text:
                        date_part = text.replace("Started Reading", "").replace("‚Äì", "").strip()
                        if date_part:
                            result['date_started'] = date_part
                            print(f"    üìÖ Found start date: {date_part}")

                    elif "Finished Reading" in text:
                        date_part = text.replace("Finished Reading", "").replace("‚Äì", "").strip()
                        if date_part:
                            result['date_ended'] = date_part
                            print(f"    üìÖ Found end date: {date_part}")

                except Exception:
                    continue

            # Fallback: search entire page text for dates
            if not result:
                try:
                    all_text = driver.find_element(By.TAG_NAME, "body").text
                    lines = all_text.split('\n')

                    for line in lines:
                        line = line.strip()
                        if "Started Reading" in line:
                            date_part = line.replace("Started Reading", "").replace("‚Äì", "").strip()
                            if date_part:
                                result['date_started'] = date_part
                        elif "Finished Reading" in line:
                            date_part = line.replace("Finished Reading", "").replace("‚Äì", "").strip()
                            if date_part:
                                result['date_ended'] = date_part
                except:
                    pass

        except Exception as e:
            print(f"    ‚ö†Ô∏è  Error extracting dates: {e}")

        # Extract book cover URL
        try:
            cover_selectors = [
                "img[id*='coverImage']",
                ".BookCover img",
                ".bookCover img",
                ".leftContainer img",
                "img[src*='book']",
                ".editionCover img"
            ]

            cover_url = None
            for selector in cover_selectors:
                try:
                    cover_element = driver.find_element(By.CSS_SELECTOR, selector)
                    cover_url = cover_element.get_attribute('src')

                    # Validate that it's actually a book cover URL
                    if cover_url and ('book' in cover_url.lower() or 'cover' in cover_url.lower()):
                        # Try to get higher resolution version
                        if '_SX' in cover_url or '_SY' in cover_url:
                            cover_url = cover_url.replace('_SX50_', '_SX500_').replace('_SY75_', '_SY500_')

                        result['cover_url'] = cover_url
                        print(f"    üñºÔ∏è  Found cover URL")
                        break
                except:
                    continue

        except Exception as e:
            print(f"    ‚ö†Ô∏è  Error extracting cover: {e}")

        return result

    except Exception as e:
        print(f"    ‚ùå Error in detailed extraction: {e}")
        return {}


def scrape_missing_reading_dates(books_needing_dates):
    """Scrape reading dates for books that don't have them in the JSON file"""
    if not books_needing_dates:
        print("‚úÖ No books need date scraping")
        return {}

    print(f"ü§ñ Starting scraping for {len(books_needing_dates)} books...")

    # Setup Chrome with persistent session
    chrome_options = Options()
    chrome_options.add_argument("--user-data-dir=/Users/valen/chrome_goodreads_profile")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    # Add headless mode option - comment out if you want to see the browser
    # chrome_options.add_argument("--headless")

    driver = webdriver.Chrome(options=chrome_options)
    scraped_data = {}

    try:
        # Navigate to reading list
        print("üîç Opening Goodreads reading list...")
        driver.get("https://www.goodreads.com/review/list/143865509?shelf=read&sort=date_read&order=d")
        time.sleep(5)

        # Check if we need to log in
        if "Sign in" in driver.title or "sign_in" in driver.current_url:
            print("üîê Please log into Goodreads...")
            print("‚è≥ Waiting for login... (you have 60 seconds)")

            start_time = time.time()
            while time.time() - start_time < 60:
                if "Sign in" not in driver.title and "sign_in" not in driver.current_url:
                    print("‚úÖ Login detected, continuing...")
                    break
                time.sleep(2)
            else:
                print("‚ùå Login timeout - continuing anyway")

        # Find all book rows
        book_rows = driver.find_elements(By.CSS_SELECTOR, "tr[id*='review_'], tr.bookalike")
        if not book_rows:
            book_rows = driver.find_elements(By.CSS_SELECTOR, "table tr")
            book_rows = [row for row in book_rows if row.find_elements(By.CSS_SELECTOR, "td")]

        print(f"Found {len(book_rows)} total book rows on page")

        books_found = 0
        for i, row in enumerate(book_rows):
            try:
                # Extract book title and find matching book
                title_element = None
                title_selectors = ["a[class*='bookTitle']", ".title a", "td.title a", "a[href*='/book/show/']"]

                for selector in title_selectors:
                    try:
                        title_element = row.find_element(By.CSS_SELECTOR, selector)
                        break
                    except:
                        continue

                if not title_element:
                    continue

                title = title_element.text.strip()

                # Check if this is one of the books we need to scrape
                matching_book_id = None
                for book_id, book_info in books_needing_dates.items():
                    if book_info['title'].lower().strip() == title.lower().strip():
                        matching_book_id = book_id
                        break

                if not matching_book_id:
                    continue

                print(f"üìñ Found book needing dates: '{title}' (ID: {matching_book_id})")

                # Look for "view" link in this row
                view_link = None
                try:
                    view_link = row.find_element(By.CSS_SELECTOR, "a[href*='#review_']")
                except:
                    try:
                        view_link = row.find_element(By.XPATH, ".//a[contains(text(), 'view')]")
                    except:
                        pass

                if view_link:
                    # Extract detailed data
                    scraped_info = extract_detailed_reading_dates_and_cover(driver, view_link, title)

                    if scraped_info:
                        scraped_data[matching_book_id] = {
                            'title': title,
                            'date_started': scraped_info.get('date_started', ''),
                            'date_ended': scraped_info.get('date_ended', ''),
                            'cover_url': scraped_info.get('cover_url', ''),
                            'scraped_at': datetime.now().isoformat()
                        }
                        books_found += 1
                        print(f"  ‚úÖ Scraped data for '{title}'")
                    else:
                        print(f"  ‚ùå Could not extract data for '{title}'")

                    # Return to main list
                    time.sleep(2)
                    driver.get("https://www.goodreads.com/review/list/143865509?shelf=read&sort=date_read&order=d")
                    time.sleep(2)

                    # Stop if we found all books we were looking for
                    if books_found >= len(books_needing_dates):
                        print("‚úÖ Found all books we were looking for!")
                        break

            except Exception as e:
                print(f"Error processing row {i}: {e}")
                continue

        print(f"üéØ Successfully scraped {len(scraped_data)} out of {len(books_needing_dates)} books")
        return scraped_data

    except Exception as e:
        print(f"‚ùå Scraping failed: {e}")
        return {}

    finally:
        driver.quit()


def expand_gr_reading_split(row, columns, col):
    """Splits the rows to have one row per day, with page division"""
    # Handle missing dates
    if pd.isna(row.get('Date started')) or pd.isna(row.get('Date ended')):
        date_df = pd.DataFrame(columns=col)
        new_row = {}
        for column in columns:
            if column not in ['Date started', 'Date ended']:
                new_row[column] = row.get(column, None)
        new_row['Timestamp'] = row.get('Date started', pd.NaT)
        new_row['page_split'] = row.get('Number of Pages', 0)

        date_df = pd.concat([date_df, pd.DataFrame([new_row])], ignore_index=True)
        return date_df

    # Create date range
    try:
        dates = pd.date_range(row['Date started'], row['Date ended'], freq='D')
        date_df = pd.DataFrame({'Timestamp': dates})

        # Add all other columns except dates
        for column in columns:
            if column not in ['Date started', 'Date ended']:
                date_df[column] = row.get(column, None)

        # Calculate page split
        num_pages = row.get('Number of Pages', 0)
        if pd.notna(num_pages) and num_pages > 0:
            date_df['page_split'] = num_pages / len(dates)
        else:
            date_df['page_split'] = 0

        return date_df
    except Exception as e:
        print(f"    ‚ö†Ô∏è  Error expanding dates for book: {e}")
        # Return single row with available data
        date_df = pd.DataFrame(columns=col)
        new_row = {}
        for column in columns:
            if column not in ['Date started', 'Date ended']:
                new_row[column] = row.get(column, None)
        new_row['Timestamp'] = row.get('Date started', pd.NaT)
        new_row['page_split'] = row.get('Number of Pages', 0)

        date_df = pd.concat([date_df, pd.DataFrame([new_row])], ignore_index=True)
        return date_df


def create_goodreads_file():
    """Main processing logic using JSON-based reading dates"""
    print("‚öôÔ∏è  Processing Goodreads data with JSON-based dates...")

    try:
        # Load Goodreads export
        df = pd.read_csv("files/exports/goodreads_exports/gr_export.csv")
        print(f"üìä Loaded {len(df)} books from Goodreads export")

        # Load existing reading dates from JSON
        dates_data = load_reading_dates_json()

        # Filter to only 'read' books
        read_books = df[df['Exclusive Shelf'] == 'read'].copy()
        print(f"üìö Found {len(read_books)} books marked as 'read'")

        # Identify books that need date scraping
        books_needing_dates = {}
        books_with_dates = {}

        for _, book in read_books.iterrows():
            book_id = str(book['Book Id'])
            title = str(book['Title']).strip()

            if book_id in dates_data and dates_data[book_id].get('date_started') and dates_data[book_id].get('date_ended'):
                # We have complete data for this book
                books_with_dates[book_id] = dates_data[book_id]
            else:
                # We need to scrape dates for this book
                books_needing_dates[book_id] = {
                    'title': title,
                    'book_data': book
                }

        print(f"‚úÖ {len(books_with_dates)} books already have dates")
        print(f"üîç {len(books_needing_dates)} books need date scraping")

        # Scrape missing dates if needed
        if books_needing_dates:
            should_scrape = input(f"Would you like to scrape dates for {len(books_needing_dates)} books? (y/N): ").lower()

            if should_scrape == 'y':
                scraped_data = scrape_missing_reading_dates(books_needing_dates)

                # Merge scraped data into main dates_data
                for book_id, scraped_info in scraped_data.items():
                    dates_data[book_id] = scraped_info
                    books_with_dates[book_id] = scraped_info

                # Save updated JSON
                save_reading_dates_json(dates_data)
                print(f"‚úÖ Updated JSON with {len(scraped_data)} newly scraped books")
            else:
                print("‚è≠Ô∏è  Skipping scraping, processing with available data...")

        # Now process all books with available dates
        processed_books = []

        for _, book in read_books.iterrows():
            book_id = str(book['Book Id'])

            if book_id in books_with_dates:
                # Add date information to book data
                date_info = books_with_dates[book_id]

                book_with_dates = book.copy()
                book_with_dates['Date started'] = pd.to_datetime(date_info.get('date_started'), errors='coerce')
                book_with_dates['Date ended'] = pd.to_datetime(date_info.get('date_ended'), errors='coerce')
                book_with_dates['cover_url'] = date_info.get('cover_url', '')

                processed_books.append(book_with_dates)
            else:
                # No dates available, add with NaT dates
                book_without_dates = book.copy()
                book_without_dates['Date started'] = pd.NaT
                book_without_dates['Date ended'] = pd.NaT
                book_without_dates['cover_url'] = ''

                processed_books.append(book_without_dates)

        # Convert to DataFrame
        df_with_dates = pd.DataFrame(processed_books)

        # Calculate reading duration and fiction classification
        df_with_dates['reading_duration'] = (df_with_dates['Date ended'] - df_with_dates['Date started']).dt.days + 1
        df_with_dates['Fiction_yn'] = df_with_dates['Bookshelves'].apply(
            lambda x: "fiction" if str(x).lower() in fiction_genres else "non-fiction"
        )

        # Prepare for expansion
        base_columns = ['Book Id', 'Title', 'Author', 'Original Publication Year', 'My Rating',
                       'Average Rating', 'Bookshelves', 'Fiction_yn', 'reading_duration',
                       'Number of Pages', 'Date started', 'Date ended', 'cover_url']

        # Only keep existing columns
        existing_columns = [col for col in base_columns if col in df_with_dates.columns]
        df_limited = df_with_dates[existing_columns]

        # Prepare final columns
        final_columns = [col for col in existing_columns if col not in ['Date started', 'Date ended']]
        final_columns.extend(['Timestamp', 'page_split'])

        # Expand each book into daily reading records
        expanded_df = pd.DataFrame(columns=final_columns)

        print(f"üìñ Expanding {len(df_limited)} books into daily reading records...")

        for _, row in df_limited.iterrows():
            try:
                expanded_row = expand_gr_reading_split(row, existing_columns, final_columns)
                expanded_df = pd.concat([expanded_df, expanded_row], ignore_index=True)
            except Exception as e:
                print(f"‚ö†Ô∏è  Error processing book '{row.get('Title', 'Unknown')}': {e}")
                continue

        # Add final required columns
        expanded_df['Seconds'] = np.nan
        expanded_df['Source'] = 'GoodReads'

        # Clean up genre column name
        if 'Bookshelves' in expanded_df.columns:
            expanded_df = expanded_df.rename(columns={'Bookshelves': 'Genre'})

        # Clean up title column
        if 'Title' in expanded_df.columns:
            expanded_df['Title'] = expanded_df['Title'].apply(lambda x: str(x).strip())

        # Save processed file
        output_path = 'files/processed_files/books/gr_processed.csv'
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        expanded_df.to_csv(output_path, sep='|', index=False)

        print(f"‚úÖ Successfully processed {len(expanded_df)} reading records")
        print(f"üìä Final columns: {list(expanded_df.columns)}")
        print(f"üìö Books with complete dates: {len(books_with_dates)}")
        print(f"üìà Books processed: {len(processed_books)}")

        return True

    except Exception as e:
        print(f"‚ùå Error processing Goodreads data: {e}")
        import traceback
        traceback.print_exc()
        return False


def upload_goodreads_results():
    """Upload the processed Goodreads files to Google Drive"""
    print("‚òÅÔ∏è  Uploading Goodreads results to Google Drive...")

    files_to_upload = [
        'files/processed_files/books/gr_processed.csv',
        'files/work_files/gr_work_files/reading_dates.json'
    ]

    # Filter to only existing files
    existing_files = [f for f in files_to_upload if os.path.exists(f)]

    if not existing_files:
        print("‚ùå No files found to upload")
        return False

    print(f"üì§ Uploading {len(existing_files)} files...")
    success = upload_multiple_files(existing_files)

    if success:
        print("‚úÖ Goodreads results uploaded successfully!")
    else:
        print("‚ùå Some files failed to upload")

    return success


def process_goodreads_export(upload="Y"):
    """Legacy function for backward compatibility"""
    if upload == "Y":
        return full_goodreads_pipeline(auto_full=True)
    else:
        return create_goodreads_file()


def full_goodreads_pipeline(auto_full=False):
    """Complete Goodreads pipeline with JSON-based processing"""
    print("\n" + "="*60)
    print("üìö GOODREADS DATA PIPELINE (JSON-BASED)")
    print("="*60)

    # First, migrate any existing Excel data to JSON
    migrate_excel_to_json()

    if auto_full:
        print("ü§ñ Auto mode: Running full pipeline...")
        choice = "1"
    else:
        print("\nSelect an option:")
        print("1. Full pipeline (download ‚Üí process ‚Üí upload)")
        print("2. Process only (use existing data)")
        print("3. Scrape dates only (for books missing dates)")

        choice = input("\nEnter your choice (1-3): ").strip()

    success = False

    if choice == "1":
        print("\nüöÄ Starting full Goodreads pipeline...")

        # Step 1: Download
        download_success = download_goodreads_data()

        # Step 2: Move files
        if download_success:
            move_success = move_goodreads_files()
        else:
            print("‚ö†Ô∏è  Download not confirmed, checking for existing files...")
            move_success = move_goodreads_files()

        # Step 3: Process (includes scraping if needed)
        if move_success or os.path.exists("files/exports/goodreads_exports/gr_export.csv"):
            process_success = create_goodreads_file()
        else:
            print("‚ùå No Goodreads export file found")
            process_success = False

        # Step 4: Upload
        if process_success:
            success = upload_goodreads_results()
        else:
            print("‚ùå Processing failed, skipping upload")
            success = False

    elif choice == "2":
        print("\n‚öôÔ∏è  Processing existing data...")
        process_success = create_goodreads_file()
        if process_success:
            success = upload_goodreads_results()
        else:
            print("‚ùå Processing failed")
            success = False

    elif choice == "3":
        print("\nüîç Scraping dates for books missing dates...")

        # Load current data to identify missing dates
        try:
            df = pd.read_csv("files/exports/goodreads_exports/gr_export.csv")
            dates_data = load_reading_dates_json()
            read_books = df[df['Exclusive Shelf'] == 'read']

            books_needing_dates = {}
            for _, book in read_books.iterrows():
                book_id = str(book['Book Id'])
                title = str(book['Title']).strip()

                if book_id not in dates_data or not dates_data[book_id].get('date_started'):
                    books_needing_dates[book_id] = {
                        'title': title,
                        'book_data': book
                    }

            if books_needing_dates:
                scraped_data = scrape_missing_reading_dates(books_needing_dates)

                # Update JSON with scraped data
                for book_id, scraped_info in scraped_data.items():
                    dates_data[book_id] = scraped_info

                success = save_reading_dates_json(dates_data)
                if success:
                    print(f"‚úÖ Updated {len(scraped_data)} books with scraped dates")
            else:
                print("‚úÖ No books need date scraping")
                success = True

        except Exception as e:
            print(f"‚ùå Error during scraping: {e}")
            success = False

    else:
        print("‚ùå Invalid choice. Please select 1-3.")
        return False

    # Final status
    print("\n" + "="*60)
    if success:
        print("‚úÖ Goodreads pipeline completed successfully!")
    else:
        print("‚ùå Goodreads pipeline failed")
    print("="*60)

    return success





if __name__ == "__main__":
    # Allow running this file directly
    print("üìö Enhanced Goodreads Processing Tool")
    print("This tool downloads, scrapes, processes, and uploads Goodreads data.")

    # Test drive connection first
    if not verify_drive_connection():
        print("‚ö†Ô∏è  Warning: Google Drive connection issues detected")
        proceed = input("Continue anyway? (Y/N): ").upper() == 'Y'
        if not proceed:
            exit()

    # Run the pipeline
    full_goodreads_pipeline(auto_full=False)
